{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7603918e",
   "metadata": {},
   "source": [
    "# Determining Sentence Sentiment using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721e976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments on hardware type: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# My libraries\n",
    "from src import Trainer, Plotter, DynamicRNN\n",
    "from src import get_sst_data_loaders\n",
    "\n",
    "# Check Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running experiments on hardware type: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc614315",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Pre-trained Vector Path\n",
    "VECTOR_PATH = \"./data/vector.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6ca033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Data Pipeline...\n",
      "--> Initializing Fields...\n",
      "--> Loading SST Splits...\n",
      "    Training samples: 8544\n",
      "--> Building Vocabulary...\n",
      "    Vocab Size: 18280\n",
      "    Label Map: {'very negative': 1, 'negative': 2, 'neutral': 3, 'positive': 4, 'very positive': 5}\n",
      "--> Building Iterators...\n",
      "Data ready. Vocab Size: 18280, Pad Index: 1\n",
      "Embeddings shape: torch.Size([18280, 300])\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Data Pipeline...\")\n",
    "\n",
    "# 1. Call the new loader function\n",
    "train_iter, val_iter, test_iter, TEXT_FIELD, LABEL_FIELD = get_sst_data_loaders(\n",
    "    batch_size=BATCH_SIZE, vector_path=VECTOR_PATH, device=DEVICE\n",
    ")\n",
    "\n",
    "# 2. Extract Metadata from the returned Fields\n",
    "VOCAB_SIZE = len(TEXT_FIELD.vocab)\n",
    "\n",
    "# --- FIX: Access pad_token from the vocab object, not the field object ---\n",
    "PAD_IDX = TEXT_FIELD.vocab.stoi.get(TEXT_FIELD.vocab.pad_token)\n",
    "\n",
    "PRETRAINED_VECTORS = TEXT_FIELD.vocab.vectors\n",
    "\n",
    "print(f\"Data ready. Vocab Size: {VOCAB_SIZE}, Pad Index: {PAD_IDX}\")\n",
    "if PRETRAINED_VECTORS is not None:\n",
    "    print(f\"Embeddings shape: {PRETRAINED_VECTORS.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d631e47",
   "metadata": {},
   "source": [
    "### Part 1: Find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c73aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- Naive RNN ---\n",
    "    {\n",
    "        \"name\": \"1_RNN_Random\",\n",
    "        \"rnn_type\": \"rnn\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2_RNN_Pretrained\",\n",
    "        \"rnn_type\": \"rnn\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Naive LSTM ---\n",
    "    {\n",
    "        \"name\": \"3_LSTM_Random\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"4_LSTM_Pretrained\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Naive GRU ---\n",
    "    {\n",
    "        \"name\": \"5_GRU_Random\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"6_GRU_Pretrained\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Better LSTM (Bi-Directional, 2 Layers) ---\n",
    "    {\n",
    "        \"name\": \"7_BiLSTM_Deep_Random\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"8_BiLSTM_Deep_Pretrained\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Better GRU (Bi-Directional, 2 Layers) ---\n",
    "    {\n",
    "        \"name\": \"9_BiGRU_Deep_Random\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"10_BiGRU_Deep_Pretrained\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51142c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Running: 1_RNN_Random ====================\n",
      "Starting 1_RNN_Random | Params: 5,628,133\n",
      "  Epoch: 01 | Time: 0m 11s | Train Acc: 0.246 | Val Acc: 0.289\n",
      "  Epoch: 02 | Time: 0m 9s | Train Acc: 0.269 | Val Acc: 0.272\n",
      "  Epoch: 03 | Time: 0m 9s | Train Acc: 0.302 | Val Acc: 0.301\n",
      "  Epoch: 04 | Time: 0m 11s | Train Acc: 0.335 | Val Acc: 0.319\n",
      "  Epoch: 05 | Time: 0m 10s | Train Acc: 0.373 | Val Acc: 0.238\n",
      "  Epoch: 06 | Time: 0m 10s | Train Acc: 0.393 | Val Acc: 0.338\n",
      "  Epoch: 07 | Time: 0m 11s | Train Acc: 0.422 | Val Acc: 0.281\n",
      "  Epoch: 08 | Time: 0m 12s | Train Acc: 0.451 | Val Acc: 0.323\n",
      "  Epoch: 09 | Time: 0m 12s | Train Acc: 0.477 | Val Acc: 0.302\n",
      "  Epoch: 10 | Time: 0m 12s | Train Acc: 0.501 | Val Acc: 0.334\n",
      "  Epoch: 11 | Time: 0m 11s | Train Acc: 0.523 | Val Acc: 0.316\n",
      "  Epoch: 12 | Time: 0m 11s | Train Acc: 0.545 | Val Acc: 0.343\n",
      "  Epoch: 13 | Time: 0m 13s | Train Acc: 0.566 | Val Acc: 0.315\n",
      "  Epoch: 14 | Time: 0m 10s | Train Acc: 0.586 | Val Acc: 0.323\n",
      "  Epoch: 15 | Time: 0m 10s | Train Acc: 0.601 | Val Acc: 0.332\n",
      "Experiement Complete with Train Accuracy: 0.6012, Val Accuracy: 0.3321\n",
      "\n",
      "==================== Running: 2_RNN_Pretrained ====================\n",
      "Loading vectors...\n",
      "Starting 2_RNN_Pretrained | Params: 5,628,133\n",
      "  Epoch: 01 | Time: 0m 10s | Train Acc: 0.330 | Val Acc: 0.324\n",
      "  Epoch: 02 | Time: 0m 11s | Train Acc: 0.387 | Val Acc: 0.388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 4. Train\u001b[39;00m\n\u001b[1;32m     38\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, DEVICE)\n\u001b[0;32m---> 39\u001b[0m history, summary \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 5. Store Data\u001b[39;00m\n\u001b[1;32m     50\u001b[0m all_histories[config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m history\n",
      "File \u001b[0;32m~/tsinghua/Intro-to-Deep-Learning/a4/src/trainer.py:101\u001b[0m, in \u001b[0;36mTrainer.run_experiment\u001b[0;34m(self, train_iter, val_iter, epochs, lr, name, hyperparameters, save_weights, save_dir)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     99\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 101\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(val_iter, criterion)\n\u001b[1;32m    104\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/tsinghua/Intro-to-Deep-Learning/a4/src/trainer.py:41\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m     39\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_accuracy(predictions, labels)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     44\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4-local-cpu/lib/python3.8/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4-local-cpu/lib/python3.8/site-packages/torch/autograd/__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store all results here\n",
    "all_histories = {}  # Key: Model Name, Value: History Dict\n",
    "all_summaries = []  # List of Summary Dicts for DataFrame\n",
    "\n",
    "for config in experiments_config:\n",
    "    print(f\"\\n{'='*20} Running: {config['name']} {'='*20}\")\n",
    "\n",
    "    # 1. Init Model\n",
    "    model = DynamicRNN(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        n_layers=config[\"n_layers\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=PAD_IDX,\n",
    "        rnn_type=config[\"rnn_type\"],\n",
    "    )\n",
    "\n",
    "    # 2. Embeddings\n",
    "    if config[\"use_pretrained\"]:\n",
    "        if PRETRAINED_VECTORS is not None:\n",
    "            print(f\"Loading vectors...\")\n",
    "            model.embedding.weight.data.copy_(PRETRAINED_VECTORS)\n",
    "        else:\n",
    "            print(\"Vectors not found, using random.\")\n",
    "\n",
    "    # 3. Hyperparams for record keeping\n",
    "    hyperparams = {\n",
    "        \"Type\": config[\"rnn_type\"].upper(),\n",
    "        \"BiDir\": config[\"bidirectional\"],\n",
    "        \"Embeds\": \"Pretrained\" if config[\"use_pretrained\"] else \"Random\",\n",
    "        \"Layers\": config[\"n_layers\"],\n",
    "    }\n",
    "\n",
    "    # 4. Train\n",
    "    trainer = Trainer(model, DEVICE)\n",
    "    history, summary = trainer.run_experiment(\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        name=config[\"name\"],\n",
    "        hyperparameters=hyperparams,\n",
    "        save_weights=True,\n",
    "    )\n",
    "\n",
    "    # 5. Store Data\n",
    "    all_histories[config[\"name\"]] = history\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nAll models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb696c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating plots for {len(all_histories)} models...\\n\")\n",
    "\n",
    "for model_name, history in all_histories.items():\n",
    "    Plotter.plot_history(history, title=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd728bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_summaries)\n",
    "\n",
    "cols = [\n",
    "    \"Model\",\n",
    "    \"Type\",\n",
    "    \"BiDir\",\n",
    "    \"Embeds\",\n",
    "    \"Best Val Acc\",\n",
    "    \"Best Val Loss\",\n",
    "    \"Time (s)\",\n",
    "    \"Parameters\",\n",
    "]\n",
    "cols = [c for c in cols if c in df_results.columns]\n",
    "df_results = df_results[cols + [c for c in df_results.columns if c not in cols]]\n",
    "\n",
    "display(df_results.sort_values(by=\"Best Val Acc\", ascending=False))\n",
    "\n",
    "csv_path = \"sst_experiment_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1779014",
   "metadata": {},
   "source": [
    "## How can we improve our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aadfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3_experiments = [\n",
    "    # The winner from Phase 2 (Frozen, No Attention)\n",
    "    {\n",
    "        \"name\": \"Baseline_Winner\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": False,\n",
    "        \"freeze_embeddings\": True,\n",
    "    },\n",
    "    # Optimization 1: Unfreeze Embeddings (Fine-Tuning)\n",
    "    {\n",
    "        \"name\": \"FineTuned_Embeddings\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": False,\n",
    "        \"freeze_embeddings\": False,  # <--- Unlocked\n",
    "    },\n",
    "    # Optimization 2: Add Attention (with frozen embeddings)\n",
    "    {\n",
    "        \"name\": \"Attention_Frozen\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": True,\n",
    "        \"freeze_embeddings\": True,  # <--- Attention ON\n",
    "    },\n",
    "    # Optimization 3: The \"Kitchen Sink\" (Attention + Fine-Tuning)\n",
    "    {\n",
    "        \"name\": \"Best_Model_Full_Opt\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": True,\n",
    "        \"freeze_embeddings\": False,  # <--- Both ON\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histories = {}  # Key: Model Name, Value: History Dict\n",
    "all_summaries = []  # List of Summary Dicts for DataFrame\n",
    "\n",
    "for config in experiments_config:\n",
    "    print(f\"\\n{'='*20} Running: {config['name']} {'='*20}\")\n",
    "\n",
    "    # 1. Init Model\n",
    "    # Note: We strictly force attention=False and freeze=True for valid Phase 1/2 comparison\n",
    "    model = DynamicRNN(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        n_layers=config[\"n_layers\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=PAD_IDX,\n",
    "        rnn_type=config[\"rnn_type\"],\n",
    "        use_attention=False,  # Optimization reserved for Phase 3\n",
    "        freeze_embeddings=True,  # We want to compare Random vs Static Pretrained first\n",
    "    )\n",
    "\n",
    "    # 2. Embeddings\n",
    "    if config[\"use_pretrained\"]:\n",
    "        if PRETRAINED_VECTORS is not None:\n",
    "            print(f\"Loading vectors...\")\n",
    "            model.embedding.weight.data.copy_(PRETRAINED_VECTORS)\n",
    "        else:\n",
    "            print(\"Vectors not found, using random.\")\n",
    "\n",
    "    # 3. Hyperparams for record keeping\n",
    "    hyperparams = {\n",
    "        \"Type\": config[\"rnn_type\"].upper(),\n",
    "        \"BiDir\": config[\"bidirectional\"],\n",
    "        \"Embeds\": \"Pretrained\" if config[\"use_pretrained\"] else \"Random\",\n",
    "        \"Layers\": config[\"n_layers\"],\n",
    "    }\n",
    "\n",
    "    # 4. Train\n",
    "    trainer = Trainer(model, DEVICE)\n",
    "    history, summary = trainer.run_experiment(\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        name=config[\"name\"],\n",
    "        hyperparameters=hyperparams,\n",
    "        save_weights=True,\n",
    "    )\n",
    "\n",
    "    # 5. Store Data\n",
    "    all_histories[config[\"name\"]] = history\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nAll models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Visualize and Report ---\n",
    "print(f\"Generating plots for {len(all_histories)} models...\\n\")\n",
    "\n",
    "# You can adjust grid size or plotting logic here\n",
    "for model_name, history in all_histories.items():\n",
    "    Plotter.plot_history(history, title=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2943fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Display Dataframe\n",
    "df_results = pd.DataFrame(all_summaries)\n",
    "\n",
    "# Reorder columns for readability\n",
    "cols = [\n",
    "    \"Model\",\n",
    "    \"Type\",\n",
    "    \"BiDir\",\n",
    "    \"Embeds\",\n",
    "    \"Best Val Acc\",\n",
    "    \"Best Val Loss\",\n",
    "    \"Time (s)\",\n",
    "    \"Parameters\",\n",
    "]\n",
    "# Filter to ensure columns exist before selecting\n",
    "cols = [c for c in cols if c in df_results.columns]\n",
    "df_results = df_results[cols + [c for c in df_results.columns if c not in cols]]\n",
    "\n",
    "# Display Leaderboard\n",
    "print(\"\\n--- Experiment Leaderboard (Sorted by Validation Accuracy) ---\")\n",
    "display(df_results.sort_values(by=\"Best Val Acc\", ascending=False))\n",
    "\n",
    "# Export\n",
    "csv_path = \"sst_phase1_2_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4-local-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
