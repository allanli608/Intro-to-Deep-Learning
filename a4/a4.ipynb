{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7603918e",
   "metadata": {},
   "source": [
    "# Determining Sentence Sentiment using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# My libraries\n",
    "from src import Trainer, SSTDataPipeline, Plotter, DynamicRNN\n",
    "\n",
    "# Check Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running experiments on hardware type: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc614315",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Pre-trained Vector Path\n",
    "VECTOR_PATH = \"./data/vector.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ca033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Data Pipeline...\")\n",
    "data_pipe = SSTDataPipeline(\n",
    "    vector_path=VECTOR_PATH, batch_size=BATCH_SIZE, device=DEVICE\n",
    ")\n",
    "train_iter, val_iter, test_iter = data_pipe.run()\n",
    "\n",
    "VOCAB_SIZE = data_pipe.vocab_size\n",
    "PAD_IDX = data_pipe.get_pad_idx()\n",
    "PRETRAINED_VECTORS = data_pipe.get_embeddings()\n",
    "\n",
    "print(f\"Data ready. Vocab Size: {VOCAB_SIZE}, Pad Index: {PAD_IDX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d631e47",
   "metadata": {},
   "source": [
    "### Part 1: Find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- Naive RNN ---\n",
    "    {\n",
    "        \"name\": \"1_RNN_Random\",\n",
    "        \"rnn_type\": \"rnn\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2_RNN_Pretrained\",\n",
    "        \"rnn_type\": \"rnn\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Naive LSTM ---\n",
    "    {\n",
    "        \"name\": \"3_LSTM_Random\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"4_LSTM_Pretrained\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Naive GRU ---\n",
    "    {\n",
    "        \"name\": \"5_GRU_Random\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"6_GRU_Pretrained\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Better LSTM (Bi-Directional, 2 Layers) ---\n",
    "    {\n",
    "        \"name\": \"7_BiLSTM_Deep_Random\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"8_BiLSTM_Deep_Pretrained\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "    # --- Better GRU (Bi-Directional, 2 Layers) ---\n",
    "    {\n",
    "        \"name\": \"9_BiGRU_Deep_Random\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"10_BiGRU_Deep_Pretrained\",\n",
    "        \"rnn_type\": \"gru\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51142c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results here\n",
    "all_histories = {}  # Key: Model Name, Value: History Dict\n",
    "all_summaries = []  # List of Summary Dicts for DataFrame\n",
    "\n",
    "for config in experiments_config:\n",
    "    print(f\"\\n{'='*20} Running: {config['name']} {'='*20}\")\n",
    "\n",
    "    # 1. Init Model\n",
    "    model = DynamicRNN(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        n_layers=config[\"n_layers\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=PAD_IDX,\n",
    "        rnn_type=config[\"rnn_type\"],\n",
    "    )\n",
    "\n",
    "    # 2. Embeddings\n",
    "    if config[\"use_pretrained\"]:\n",
    "        if PRETRAINED_VECTORS is not None:\n",
    "            print(f\"Loading vectors...\")\n",
    "            model.embedding.weight.data.copy_(PRETRAINED_VECTORS)\n",
    "        else:\n",
    "            print(\"Vectors not found, using random.\")\n",
    "\n",
    "    # 3. Hyperparams for record keeping\n",
    "    hyperparams = {\n",
    "        \"Type\": config[\"rnn_type\"].upper(),\n",
    "        \"BiDir\": config[\"bidirectional\"],\n",
    "        \"Embeds\": \"Pretrained\" if config[\"use_pretrained\"] else \"Random\",\n",
    "        \"Layers\": config[\"n_layers\"],\n",
    "    }\n",
    "\n",
    "    # 4. Train\n",
    "    trainer = Trainer(model, DEVICE)\n",
    "    history, summary = trainer.run_experiment(\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        name=config[\"name\"],\n",
    "        hyperparameters=hyperparams,\n",
    "        save_weights=True,\n",
    "    )\n",
    "\n",
    "    # 5. Store Data\n",
    "    all_histories[config[\"name\"]] = history\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nAll models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb696c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating plots for {len(all_histories)} models...\\n\")\n",
    "\n",
    "for model_name, history in all_histories.items():\n",
    "    Plotter.plot_history(history, title=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd728bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_summaries)\n",
    "\n",
    "cols = [\n",
    "    \"Model\",\n",
    "    \"Type\",\n",
    "    \"BiDir\",\n",
    "    \"Embeds\",\n",
    "    \"Best Val Acc\",\n",
    "    \"Best Val Loss\",\n",
    "    \"Time (s)\",\n",
    "    \"Parameters\",\n",
    "]\n",
    "cols = [c for c in cols if c in df_results.columns]\n",
    "df_results = df_results[cols + [c for c in df_results.columns if c not in cols]]\n",
    "\n",
    "display(df_results.sort_values(by=\"Best Val Acc\", ascending=False))\n",
    "\n",
    "csv_path = \"sst_experiment_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1779014",
   "metadata": {},
   "source": [
    "## How can we improve our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aadfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3_experiments = [\n",
    "    # The winner from Phase 2 (Frozen, No Attention)\n",
    "    {\n",
    "        \"name\": \"Baseline_Winner\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": False,\n",
    "        \"freeze_embeddings\": True,\n",
    "    },\n",
    "    # Optimization 1: Unfreeze Embeddings (Fine-Tuning)\n",
    "    {\n",
    "        \"name\": \"FineTuned_Embeddings\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": False,\n",
    "        \"freeze_embeddings\": False,  # <--- Unlocked\n",
    "    },\n",
    "    # Optimization 2: Add Attention (with frozen embeddings)\n",
    "    {\n",
    "        \"name\": \"Attention_Frozen\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": True,\n",
    "        \"freeze_embeddings\": True,  # <--- Attention ON\n",
    "    },\n",
    "    # Optimization 3: The \"Kitchen Sink\" (Attention + Fine-Tuning)\n",
    "    {\n",
    "        \"name\": \"Best_Model_Full_Opt\",\n",
    "        \"rnn_type\": \"lstm\",\n",
    "        \"n_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_pretrained\": True,\n",
    "        \"use_attention\": True,\n",
    "        \"freeze_embeddings\": False,  # <--- Both ON\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histories = {}  # Key: Model Name, Value: History Dict\n",
    "all_summaries = []  # List of Summary Dicts for DataFrame\n",
    "\n",
    "for config in experiments_config:\n",
    "    print(f\"\\n{'='*20} Running: {config['name']} {'='*20}\")\n",
    "\n",
    "    # 1. Init Model\n",
    "    # Note: We strictly force attention=False and freeze=True for valid Phase 1/2 comparison\n",
    "    model = DynamicRNN(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        n_layers=config[\"n_layers\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=PAD_IDX,\n",
    "        rnn_type=config[\"rnn_type\"],\n",
    "        use_attention=False,  # Optimization reserved for Phase 3\n",
    "        freeze_embeddings=True,  # We want to compare Random vs Static Pretrained first\n",
    "    )\n",
    "\n",
    "    # 2. Embeddings\n",
    "    if config[\"use_pretrained\"]:\n",
    "        if PRETRAINED_VECTORS is not None:\n",
    "            print(f\"Loading vectors...\")\n",
    "            model.embedding.weight.data.copy_(PRETRAINED_VECTORS)\n",
    "        else:\n",
    "            print(\"Vectors not found, using random.\")\n",
    "\n",
    "    # 3. Hyperparams for record keeping\n",
    "    hyperparams = {\n",
    "        \"Type\": config[\"rnn_type\"].upper(),\n",
    "        \"BiDir\": config[\"bidirectional\"],\n",
    "        \"Embeds\": \"Pretrained\" if config[\"use_pretrained\"] else \"Random\",\n",
    "        \"Layers\": config[\"n_layers\"],\n",
    "    }\n",
    "\n",
    "    # 4. Train\n",
    "    trainer = Trainer(model, DEVICE)\n",
    "    history, summary = trainer.run_experiment(\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        name=config[\"name\"],\n",
    "        hyperparameters=hyperparams,\n",
    "        save_weights=True,\n",
    "    )\n",
    "\n",
    "    # 5. Store Data\n",
    "    all_histories[config[\"name\"]] = history\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nAll models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Visualize and Report ---\n",
    "print(f\"Generating plots for {len(all_histories)} models...\\n\")\n",
    "\n",
    "# You can adjust grid size or plotting logic here\n",
    "for model_name, history in all_histories.items():\n",
    "    Plotter.plot_history(history, title=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2943fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Display Dataframe\n",
    "df_results = pd.DataFrame(all_summaries)\n",
    "\n",
    "# Reorder columns for readability\n",
    "cols = [\n",
    "    \"Model\",\n",
    "    \"Type\",\n",
    "    \"BiDir\",\n",
    "    \"Embeds\",\n",
    "    \"Best Val Acc\",\n",
    "    \"Best Val Loss\",\n",
    "    \"Time (s)\",\n",
    "    \"Parameters\",\n",
    "]\n",
    "# Filter to ensure columns exist before selecting\n",
    "cols = [c for c in cols if c in df_results.columns]\n",
    "df_results = df_results[cols + [c for c in df_results.columns if c not in cols]]\n",
    "\n",
    "# Display Leaderboard\n",
    "print(\"\\n--- Experiment Leaderboard (Sorted by Validation Accuracy) ---\")\n",
    "display(df_results.sort_values(by=\"Best Val Acc\", ascending=False))\n",
    "\n",
    "# Export\n",
    "csv_path = \"sst_phase1_2_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4-local-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
